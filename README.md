# DGLGraphParallel

PyTorch data parallelism solution for training DGL(Deep Graph Library) models on Single-Machine-Multi-GPUs.

### Getting Started

Since DGL doesn't provide simple APIs for training DGL by Single-Machine-Multi-GPUs, this module provides an unofficial way to support training GCN-like modules on multi-GPUs with PyTorch backend.

### Prerequisites

* PyTorch 1.2.x (cuda enabled)

* DGL 0.3.x

### APIs

```python
class DGLNodeFlowLoader
```
This class generates multi-nodeflows according to the cuda device number. The genereated nodeflows will be gathered into a list as the input data.

Currently it only supports `NeighborSampler`

```python
class DGLGraphDataParallel(torch.nn.Module)
```
Similar to `torch.nn.DataParallel`, this class automatically replicates the model across GPUs and scatters the inputs data (generated by `DGLNodeFlowLoader`) to corresponding GPUs.

### Run

See the examples of `gcn_ns_dp.py` in folder examples.

To run the example, instruction can be like as:

```sh
$ cp DGLGraphParallel/examples/gcn_ns_dp.py ./
$ DGLBACKEND=pytorch python gcn_ns_dp.py --gpu 0,1,2 --dataset reddit-self-loop --num-neighbors 10 --batch-size 30000 --test-batch-size 30000
```

### Implementation Details

```python
class DGLNodeFlowLoader
```
It will generate same number nodeflows as the `torch.cuda.device_count()` and gather them into a list for inputs. Also the `labels` will be returned as the concatenation of all nodeflows corresponded labels in the inputs.

```python
class DGLGraphDataParallel
```
This class is modified from `torch.nn.DataParallel`. The input of the model should be a list of nodeflows. 

Each `forward` will do things like below (similar with `torch.nn.DataParallel`):

  * Scatter `inputs` and `kwargs` to all GPUs (by using `NodeFlow.copy_from_parent()`)

  * Replicates the module to all GPUs (same as `torch`)

  * Parallel apply for all GPUs (same as `torch`)

  * Gather forwarding results back to one GPU (same as `torch`)

So `DGLGraphDataParallel` will transmit datas (nodeflows), weights, forwarding results at every single forwarding. The backward (gradient generation and weights update) will be only applied on one GPU.